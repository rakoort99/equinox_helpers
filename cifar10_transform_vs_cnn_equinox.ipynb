{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prelims"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import os\n",
    "from typing import Any, Sequence\n",
    "import numpy as np\n",
    "\n",
    "%load_ext tensorboard\n",
    "\n",
    "# JAX/Equinox\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import random\n",
    "import equinox as eqx\n",
    "\n",
    "# PyTorch for data loading\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "\n",
    "## Imports for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "## equinox helper funcitons\n",
    "from eqx_helpers import EqxTrainerModule, create_data_loaders\n",
    "\n",
    "sns.reset_orig()\n",
    "sns.set_theme()\n",
    "\n",
    "DATASET_PATH = os.path.join(os.getcwd(), \"data\")\n",
    "CHECKPOINT_PATH = os.path.join(os.getcwd(), \"saved_models\\\\cifar_mess\\\\\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision import transforms\n",
    "\n",
    "# Transformations applied on each image => bring them into a numpy array\n",
    "DATA_MEANS = np.array([0.49139968, 0.48215841, 0.44653091])\n",
    "DATA_STD = np.array([0.24703223, 0.24348513, 0.26158784])\n",
    "\n",
    "\n",
    "def image_to_numpy(img):\n",
    "    img = np.array(img, dtype=np.float32)\n",
    "    img = (img / 255.0 - DATA_MEANS) / DATA_STD\n",
    "    return img.transpose(2,0,1)\n",
    "\n",
    "\n",
    "def img_restore(img):\n",
    "    img = img.transpose(1,2,0)\n",
    "    img = ((img * DATA_STD) + DATA_MEANS) * 255.0\n",
    "    img = np.array(img, dtype=np.int32)\n",
    "    return img\n",
    "\n",
    "# augmentation step\n",
    "test_transform = transforms.Compose(\n",
    "    [   image_to_numpy,\n",
    "    ]\n",
    ")\n",
    "train_transform = transforms.Compose(\n",
    "    [   transforms.TrivialAugmentWide(),\n",
    "        image_to_numpy,\n",
    "    ]\n",
    ")\n",
    "\n",
    "# we don't want to augment the validation data, so we split this up\n",
    "train_dataset = CIFAR10(\n",
    "    root=DATASET_PATH, train=True, transform=train_transform, download=True\n",
    ")\n",
    "val_dataset = CIFAR10(\n",
    "    root=DATASET_PATH, train=True, transform=test_transform, download=True\n",
    ") # note that we use `test_transform here`\n",
    "\n",
    "\n",
    "# we split with the same seed, but using the transformed and untransformed data separately\n",
    "train_set, _ = data.random_split(\n",
    "    train_dataset, [45000, 5000], generator=torch.Generator().manual_seed(42)\n",
    ")\n",
    "_, val_set = data.random_split(\n",
    "    val_dataset, [45000, 5000], generator=torch.Generator().manual_seed(42)\n",
    ")\n",
    "\n",
    "# Loading the test set\n",
    "test_set = CIFAR10(\n",
    "    root=DATASET_PATH, train=False, transform=test_transform, download=True\n",
    ")\n",
    "\n",
    "#reducing the size of the data by 96% for testing:\n",
    "_, train_set = data.random_split(\n",
    "    train_set, [43200, 1800], generator=torch.Generator().manual_seed(42)\n",
    ")\n",
    "\n",
    "_, val_set = data.random_split(\n",
    "    val_set, [4800, 200], generator=torch.Generator().manual_seed(42)\n",
    ")\n",
    "_, test_set = data.random_split(\n",
    "    test_set, [9600, 400], generator=torch.Generator().manual_seed(42)\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "train_loader, val_loader, test_loader = create_data_loaders(\n",
    "    train_set, val_set, test_set, train=[True, False, False], batch_size= 200 #256\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### define model, blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormedConv_Block(eqx.Module):\n",
    "    conv: eqx.nn.LayerNorm\n",
    "    pool: eqx.nn.MaxPool2d\n",
    "    drop: eqx.nn.Dropout\n",
    "    batch_norm: eqx.nn.LayerNorm\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_shape: Sequence[int] = (3,32,32),\n",
    "        out_channels: int = 16,\n",
    "        conv_window: int = 3,\n",
    "        pooling_kernel: int = 2,\n",
    "        pooling_stride: int = 1,\n",
    "        drop_rate: int = 0.0,\n",
    "        key: jax.Array = random.key(42),\n",
    "    ):\n",
    "        \"\"\"Block with the following layers:\n",
    "        2D convolution -> 2D pooling -> RELU activation -> batch normalization\n",
    "\n",
    "        Args:\n",
    "            input_shape (Sequence[int], optional): Shape of (unbatched) input. Order is Channel, Height, Width. Defaults to (3,32,32).\n",
    "            out_channels (int, optional): Number of output channels. Defaults to 16.\n",
    "            conv_window (int, optional): Size of 2d convolutional window. Defaults to 3.\n",
    "            pooling_kernel (int, optional): Kernel for 2d max pooling. Defaults to 2.\n",
    "            pooling_stride (int, optional): Stride length for pooling. Defaults to 1.\n",
    "            drop_rate (int, optional): Dropout rate. Defaults to 0.0.\n",
    "            key (jax.Array, optional): PRNGKey. Defaults to random.key(42).\n",
    "        \"\"\"            \n",
    "        self.conv = eqx.nn.Conv2d(\n",
    "            input_shape[0], out_channels, kernel_size=conv_window, key=key\n",
    "        )\n",
    "        if pooling_kernel == pooling_stride == 1:\n",
    "            self.pool = eqx.nn.Identity()\n",
    "        else:\n",
    "            self.pool = eqx.nn.MaxPool2d(pooling_kernel, stride=pooling_stride)\n",
    "        self.drop = eqx.nn.Dropout(drop_rate)\n",
    "\n",
    "        new_H = (\n",
    "            int(((input_shape[1] - conv_window) + 1 - pooling_kernel) / pooling_stride)\n",
    "            + 1\n",
    "        )\n",
    "        input_shape = (out_channels, new_H, new_H)\n",
    "        self.batch_norm = eqx.nn.BatchNorm(out_channels, \"batch\", momentum=0.5)\n",
    "        # self.batch_norm = eqx.nn.LayerNorm(input_shape[1:])\n",
    "\n",
    "    def __call__(self, x, key, state, train=True, return_activations=False):\n",
    "        activations= []\n",
    "        x = self.conv(x)\n",
    "        activations.append(x)\n",
    "        x = self.pool(x)\n",
    "        x = jax.nn.gelu(x)\n",
    "        x = self.drop(x, key=key, inference=not train)\n",
    "        # x, state = self.batch_norm(x, state, inference=not train)\n",
    "        # x = jax.vmap(self.batch_norm)(x)\n",
    "        return (x, state, activations) if return_activations else (x, state)\n",
    "\n",
    "\n",
    "class NormedDense_Block(eqx.Module):\n",
    "    dense: eqx.nn.Linear\n",
    "    drop: eqx.nn.Dropout\n",
    "    batch_norm: eqx.nn.BatchNorm\n",
    "\n",
    "    def __init__(self, \n",
    "                 in_dim: int = 64, \n",
    "                 out_dim: int = 10, \n",
    "                 drop_rate: float = 0.0,\n",
    "                 key: jax.Array = random.key(42)):\n",
    "        \"\"\"Block with the following layers:\n",
    "        Linear NN -> RELU activation -> layer normalization\n",
    "\n",
    "        Args:\n",
    "            in_dim (int, optional): (Unbatched) input dimension. Defaults to 64.\n",
    "            out_dim (int, optional): (Unbatched) output dimension. Defaults to 10.\n",
    "            drop_rate (float, optional): Dropout rate. Defaults to 0.0.\n",
    "            key (jax.Array, optional): PRNGKey. Defaults to random.key(42).\n",
    "        \"\"\"        \n",
    "        self.dense = eqx.nn.Linear(in_dim, out_dim, key=key)\n",
    "        self.drop = eqx.nn.Dropout(drop_rate)\n",
    "        # self.batch_norm = eqx.nn.BatchNorm(out_dim, \"batch\")\n",
    "        self.batch_norm = eqx.nn.LayerNorm(out_dim)\n",
    "\n",
    "    def __call__(self, x, key, state, train=True, return_activations=False):\n",
    "        activations = []\n",
    "        x = self.dense(x)\n",
    "        activations.append(x)\n",
    "        x = jax.nn.gelu(x)\n",
    "        x = self.drop(x, key=key, inference=not train)\n",
    "        # x, state = self.batch_norm(x, state, inference=not train)\n",
    "        # x = self.batch_norm(x)\n",
    "        return (x, state, activations) if return_activations else (x, state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class eqx_CNN_Classifier(eqx.Module):\n",
    "    conv_layers: Sequence[NormedConv_Block]\n",
    "    linear_layers: Sequence[NormedDense_Block]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_shape: Sequence[int] = (3, 32, 32),\n",
    "        num_classes: int = 10,\n",
    "        conv_channels: Sequence[int] = [16],\n",
    "        conv_windows: Sequence[int] = [5],\n",
    "        pooling_kernel: Sequence[int] = [2],\n",
    "        pooling_stride: Sequence[int] = [2],\n",
    "        conv_drop: float = 0.0,\n",
    "        hidden_dims: Sequence[int] = [64],\n",
    "        dense_drop: float = 0.0,\n",
    "        key: jax.Array = random.key(42),\n",
    "    ):\n",
    "        \"\"\"Classifier formed by stacking convolutional and dense blocks.\n",
    "\n",
    "        Args:\n",
    "            input_shape (Sequence[int], optional): (Unbatched) shape of input. Order is Channel, Height, Width. Defaults to (3, 32, 32).\n",
    "            num_classes (int, optional): Number of classes to predict. Shape of final dense output. Defaults to 10.\n",
    "            conv_channels (Sequence[int], optional): Number of channels for each convolutional layer. Defaults to [16].\n",
    "            conv_windows (Sequence[int], optional): Window size for each convolutional layer. Defaults to [5].\n",
    "            pooling_kernel (Sequence[int], optional): Kernel size for each convolutional layer. Defaults to [2].\n",
    "            pooling_stride (Sequence[int], optional): Stride length for each convolutional layer. Defaults to [2].\n",
    "            conv_drop (float, optional): Dropout rate for convolutional layers. Defaults to 0.0.\n",
    "            hidden_dims (Sequence[int], optional): Hidden dims for intermediate dense layers. Defaults to [64].\n",
    "            dense_drop (float, optional): Dropout for dense layers. Defaults to 0.0.\n",
    "            key (jax.Array, optional): PRNGKey. Defaults to random.key(42).\n",
    "        \"\"\"        \n",
    "        def assert_equal_length(*sequences):\n",
    "            # Calculate the lengths of all sequences\n",
    "            lengths = [len(seq) for seq in sequences]\n",
    "\n",
    "            # Check if all lengths are equal to the first length in the list\n",
    "            assert all(\n",
    "                length == lengths[0] for length in lengths\n",
    "            ), f\"Sequences defining ConvSA block are of different lengths: {lengths}\"\n",
    "\n",
    "        assert_equal_length(conv_channels, conv_windows, pooling_kernel, pooling_stride)\n",
    "        \n",
    "        # establish convulutional layers\n",
    "        self.conv_layers = []\n",
    "        key, *conv_keys = random.split(key, num=len(conv_channels) + 1)\n",
    "        conv_iterator = enumerate(\n",
    "            zip(conv_windows, conv_channels, pooling_kernel, pooling_stride, conv_keys)\n",
    "        )\n",
    "        for i, (cw, cf, pk, ps, k) in conv_iterator:\n",
    "            self.conv_layers.append(\n",
    "                NormedConv_Block(\n",
    "                    input_shape=input_shape,\n",
    "                    out_channels=cf,\n",
    "                    conv_window=cw,\n",
    "                    pooling_kernel=pk,\n",
    "                    pooling_stride=ps,\n",
    "                    key=k,\n",
    "                    drop_rate=conv_drop,\n",
    "                )\n",
    "            )\n",
    "\n",
    "            new_H = (\n",
    "                int(\n",
    "                    ((input_shape[1] - conv_windows[i]) + 1 - pooling_kernel[i])\n",
    "                    / pooling_stride[i]\n",
    "                )\n",
    "                + 1\n",
    "            )\n",
    "            input_shape = (cf, new_H, new_H)\n",
    "\n",
    "        # establish linear layers\n",
    "        self.linear_layers = []\n",
    "        key, *lin_keys = random.split(key, num=len(hidden_dims) + 1)\n",
    "        input_shape = np.prod(input_shape)\n",
    "        for i, (hidden_dim, k) in enumerate(zip(hidden_dims, lin_keys)):\n",
    "            self.linear_layers.append(\n",
    "                NormedDense_Block(\n",
    "                    in_dim=input_shape, out_dim=hidden_dim, key=k, drop_rate=dense_drop\n",
    "                )\n",
    "            )\n",
    "            input_shape = hidden_dim\n",
    "        self.linear_layers.append(\n",
    "            NormedDense_Block(\n",
    "                in_dim=input_shape, out_dim=num_classes, key=key, drop_rate=0\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def __call__(self, x, key, state, train=True, return_activations = False):\n",
    "        subkey, *conv_keys = random.split(key, num=len(self.conv_layers) + 1)\n",
    "        activations = []\n",
    "        for layer, k in zip(self.conv_layers, conv_keys):\n",
    "            outputs = layer(x, k, state=state, train=train, return_activations=return_activations)\n",
    "            x, state = outputs[0], outputs[1]\n",
    "            if return_activations:\n",
    "                activations += outputs[2]\n",
    "        x = jnp.ravel(x)\n",
    "        subkey, *lin_keys = random.split(subkey, num=len(self.linear_layers) + 1)\n",
    "        for layer, k in zip(self.linear_layers, lin_keys):\n",
    "            outputs = layer(x, k, state=state, train=train, return_activations=return_activations)\n",
    "            x, state = outputs[0], outputs[1]\n",
    "            if return_activations:\n",
    "                activations += outputs[2]\n",
    "        x = jax.nn.log_softmax(x)\n",
    "        return (x, state, activations) if return_activations else (x, state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "\n",
    "class CNNClassTrainer(EqxTrainerModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        conv_windows: Sequence[int],\n",
    "        conv_channels: Sequence[int],\n",
    "        pooling_kernel: Sequence[bool],\n",
    "        pooling_stride: Sequence[bool],\n",
    "        hidden_dims: Sequence[int],\n",
    "        input_shape: int,\n",
    "        num_classes: int,\n",
    "        conv_drop: float = 0.0,\n",
    "        dense_drop: float = 0.0,\n",
    "        trial: Any = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self.trial = trial\n",
    "        super().__init__(\n",
    "            model_class=eqx_CNN_Classifier,\n",
    "            model_hparams={\n",
    "                \"conv_windows\": conv_windows,\n",
    "                \"conv_channels\": conv_channels,\n",
    "                \"pooling_kernel\": pooling_kernel,\n",
    "                \"pooling_stride\": pooling_stride,\n",
    "                \"input_shape\": input_shape,\n",
    "                \"hidden_dims\": hidden_dims,\n",
    "                \"num_classes\": num_classes,\n",
    "                \"conv_drop\": conv_drop,\n",
    "                \"dense_drop\": dense_drop,\n",
    "            },\n",
    "            **kwargs,\n",
    "        )\n",
    "    def model_init(self, key):\n",
    "        def init_linear_weight(model, key):\n",
    "            # initializer = jax.nn.initializers.variance_scaling(2., \"fan_\", 'truncated_normal', -1, -2)\n",
    "            initializer = jax.nn.initializers.xavier_normal(-1, -2)\n",
    "            def replace(x):\n",
    "                return isinstance(x, eqx.nn.Linear)\n",
    "            def get_weights(m):\n",
    "                return [x.weight for x in jax.tree_util.tree_leaves(m, is_leaf=replace) if replace(x)]\n",
    "            weights = get_weights(model)\n",
    "            new_weights = [initializer(subkey, weight.shape)\n",
    "                            for weight, subkey in zip(weights, jax.random.split(key, len(weights)))]\n",
    "            new_model = eqx.tree_at(get_weights, model, new_weights)\n",
    "            return new_model\n",
    "        def init_conv_weight(model, key):\n",
    "            # initializer = jax.nn.initializers.variance_scaling(2., \"fan_in\", 'truncated_normal', -3, -4)\n",
    "            initializer = jax.nn.initializers.xavier_normal(-3, -4)\n",
    "            def replace(x):\n",
    "                return isinstance(x, eqx.nn.Conv2d)\n",
    "            def get_weights(m):\n",
    "                return [x.weight for x in jax.tree_util.tree_leaves(m, is_leaf=replace) if replace(x)]\n",
    "            weights = get_weights(model)\n",
    "            new_weights = [initializer(subkey, weight.shape)\n",
    "                            for weight, subkey in zip(weights, jax.random.split(key, len(weights)))]\n",
    "            new_model = eqx.tree_at(get_weights, model, new_weights)\n",
    "            return new_model\n",
    "        key1, key2 = random.split(key)\n",
    "        self.model = init_linear_weight(self.model, key1)\n",
    "        self.model = init_conv_weight(self.model, key2)\n",
    "\n",
    "    def create_functions(self):\n",
    "        def loss_function(model, x, y, key, state, train=True):\n",
    "            model_out, state = jax.vmap(\n",
    "                model,\n",
    "                axis_name=\"batch\",\n",
    "                in_axes=(0, None, None, None),\n",
    "                out_axes=(0, None),\n",
    "            )(x, key, state, train)\n",
    "            # since we use log_softmax, cross entropy is just the value of our model output\n",
    "            # indexed by the true value of y.\n",
    "            loss = jnp.take_along_axis(model_out, jnp.expand_dims(y, 1), axis=1)\n",
    "            loss = -jnp.mean(loss)\n",
    "\n",
    "            pred_y = jnp.argmax(model_out, axis=1)\n",
    "            acc = jnp.mean(y == pred_y)\n",
    "            return loss, (acc, state)\n",
    "\n",
    "        def train_step(model, opt_state, batch, key, state):\n",
    "            x, y = batch\n",
    "            (loss, (acc, state)), grads = eqx.filter_value_and_grad(\n",
    "                loss_function, has_aux=True\n",
    "            )(model, x, y, key, state)\n",
    "            updates, opt_state = self.opt.update(grads, opt_state, model)\n",
    "            model = eqx.apply_updates(model, updates)\n",
    "            metrics = {\"loss\": loss, \"acc\": acc}\n",
    "            return model, opt_state, metrics, state\n",
    "\n",
    "        def eval_step(model, batch, key, state):\n",
    "            x, y = batch\n",
    "            _, (acc, _) = loss_function(model, x, y, key, state, train=False)\n",
    "            return {\"acc\": acc}\n",
    "\n",
    "        return train_step, eval_step\n",
    "\n",
    "    def on_validation_epoch_end(self, epoch_idx, eval_metrics, val_loader):\n",
    "        if self.trial:\n",
    "            self.trial.report(eval_metrics[\"val/acc\"], step=epoch_idx)\n",
    "            if self.trial.should_prune():\n",
    "                raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    def bind_model(self):\n",
    "        \"\"\"\n",
    "        Returns a batched model with parameters bound to it. Enables an easier inference\n",
    "        access.\n",
    "\n",
    "        Returns:\n",
    "          The model with `key`, `state`, `train`, baked in, such that it only takes x input\n",
    "        \"\"\"\n",
    "\n",
    "        def mod(x):\n",
    "            model_out, state = jax.vmap(\n",
    "                self.model,\n",
    "                axis_name=\"batch\",\n",
    "                in_axes=(0, None, None, None),\n",
    "                out_axes=(0, None),\n",
    "            )(x, self.key, self.state, False)\n",
    "            return model_out\n",
    "\n",
    "        return mod\n",
    "\n",
    "    def vizualize_grads(self, batch):\n",
    "        \"\"\"Plots histogram of gradients at model leaves. \n",
    "            Interior functions are 75% copy-paste from `create_functions`\n",
    "        \"\"\"\n",
    "\n",
    "        def loss_function(model, x, y, key, state, train=True):\n",
    "            model_out, state = jax.vmap(\n",
    "                model,\n",
    "                axis_name=\"batch\",\n",
    "                in_axes=(0, None, None, None),\n",
    "                out_axes=(0, None),\n",
    "            )(x, key, state, train)\n",
    "            # since we use log_softmax, cross entropy is just the value of our model output\n",
    "            # indexed by the true value of y.\n",
    "            loss = jnp.take_along_axis(model_out, jnp.expand_dims(y, 1), axis=1)\n",
    "            loss = -jnp.mean(loss)\n",
    "\n",
    "            pred_y = jnp.argmax(model_out, axis=1)\n",
    "            acc = jnp.mean(y == pred_y)\n",
    "            return loss, (acc, state)\n",
    "\n",
    "        def get_grad_and_path(model, batch, key, state):\n",
    "            x, y = batch\n",
    "            (loss, (acc, state)), grads = eqx.filter_value_and_grad(\n",
    "                loss_function, has_aux=True\n",
    "            )(model, x, y, key, state)\n",
    "            dt = jax.tree_util.tree_leaves_with_path(grads)\n",
    "            grads, names = [], []\n",
    "            for (\n",
    "                path,\n",
    "                grad,\n",
    "            ) in (\n",
    "                dt\n",
    "            ):  # filter basic info we don't care about (bias terms, normalization)\n",
    "                if (\n",
    "                    (len(grad.shape) > 1)\n",
    "                    and (path[-1].name != \"bias\")\n",
    "                    and (not path[-2].name.endswith(\"norm\"))\n",
    "                ):\n",
    "                    grads.append(grad.reshape(-1))\n",
    "                    names.append(path[-2].name + \"_\" + path[-1].name)\n",
    "            return grads, names\n",
    "        \n",
    "        def viz_grads(grads, names, color=\"C0\"):\n",
    "            columns = len(grads)\n",
    "            fig, ax = plt.subplots(1, columns, figsize=(columns * 3.5, 2.5))\n",
    "            for g_idx, g in enumerate(grads):\n",
    "                key = f\"{names[g_idx]}\"\n",
    "                key_ax = ax[g_idx % columns]\n",
    "                sns.histplot(data=g, bins=30, ax=key_ax, color=color, kde=False, stat='probability')\n",
    "                key_ax.set_title(str(key))\n",
    "                key_ax.set_xlabel(\"Grad magnitude\")\n",
    "            fig.suptitle(\"Gradient magnitude distributions\", fontsize=14, y=1.05)\n",
    "            fig.subplots_adjust(wspace=0.45)\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "\n",
    "        grads, names = get_grad_and_path(self.model, batch, self.key, self.state)\n",
    "        viz_grads(grads, names)\n",
    "    def visualize_activations(self, batch):\n",
    "        def get_activations(model, batch, key, state):\n",
    "            x, y = batch\n",
    "            preds, state, acts = jax.vmap(\n",
    "                model,\n",
    "                axis_name=\"batch\",\n",
    "                in_axes=(0, None, None, None, None),\n",
    "                out_axes=(0, None, 0),\n",
    "            )(x, key, state, False, True)\n",
    "            acts = [jnp.reshape(a, (-1,)) for a in acts]\n",
    "            return acts\n",
    "        def viz_acts(acts, color=\"C0\"):\n",
    "\n",
    "            nrowsfn = lambda x: x // 4 + 1 if x % 4 != 0 else x // 4  # noqa: E731\n",
    "            fig, axes = plt.subplots(nrowsfn(len(acts)), 4, sharey=True)\n",
    "            ax = axes.ravel()\n",
    "            for i, g in enumerate(acts):\n",
    "                Q1 = jnp.quantile(g, .25)\n",
    "                Q3 = jnp.quantile(g, .75)\n",
    "                IQR = Q3 - Q1\n",
    "                filtered = g[(g >= (Q1 - 2. * IQR)) & (g <= (Q3 + 2. * IQR))]\n",
    "\n",
    "                key_ax = ax[i]\n",
    "                key_ax.set_title(f'var: {jnp.var(g):.2f}')\n",
    "                sns.histplot(data=filtered, bins=30, ax=key_ax, color=color, kde=False, stat='probability')\n",
    "            fig.suptitle(\"Activation magnitude distributions\",)\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "        acts = get_activations(self.model, batch, self.key, self.state)\n",
    "        viz_acts(acts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = CNNClassTrainer(\n",
    "    conv_windows=[3, 5],\n",
    "    conv_channels=[16, 32],\n",
    "    pooling_kernel=[2, 2],\n",
    "    pooling_stride=[2, 2],\n",
    "    hidden_dims=[64],\n",
    "    input_shape=(3, 32, 32),\n",
    "    num_classes=10,\n",
    "    conv_drop=0.0,\n",
    "    dense_drop=0.0,\n",
    "    optimizer_hparams={\"optimizer\": \"nadamw\", \"lr\": 5e-3},\n",
    "    logger_params={\n",
    "        \"base_log_dir\": CHECKPOINT_PATH, # model params, checkpointed models, and TensorBoard events files go here \n",
    "    },\n",
    "    check_val_every_n_epoch=5, # this means the model is checkpointed every fifth epoch\n",
    "    seed=40,\n",
    "    debug=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = trainer.train_model(train_loader, val_loader, test_loader, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Validation accuracy: {metrics[\"val/acc\"]:4.2%}')\n",
    "print(f'Test accuracy: {metrics[\"test/acc\"]:4.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_loader))\n",
    "trainer.vizualize_grads(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.visualize_activations(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm loading checkpoints works properly!\n",
    "\n",
    "imgs, y = batch\n",
    "mod = trainer.bind_model()\n",
    "\n",
    "trainer2 = CNNClassTrainer.load_from_checkpoint(os.path.join(CHECKPOINT_PATH, r'eqx_CNN_Classifier\\\\version_0') )\n",
    "mod2 = trainer2.bind_model()\n",
    "\n",
    "pred = mod(imgs).argmax(-1) # our model ends with log-softmax\n",
    "pred2 = mod2(imgs).argmax(-1)\n",
    "\n",
    "assert jnp.all(pred2 == pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((pred != y).mean())\n",
    "misclass_imgs = imgs[pred != y]\n",
    "bad_pred = pred[pred != y]\n",
    "real_answer = y[pred != y]\n",
    "\n",
    "labels = [\n",
    "    \"airplane\",\n",
    "    \"automobile\",\n",
    "    \"bird\",\n",
    "    \"cat\",\n",
    "    \"deer\",\n",
    "    \"dog\",\n",
    "    \"frog\",\n",
    "    \"horse\",\n",
    "    \"ship\",\n",
    "    \"truck\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 2\n",
    "print(f\"predicted: {labels[bad_pred[i]]}\")\n",
    "print(f\"real: {labels[real_answer[i]]}\")\n",
    "plt.imshow(img_restore(misclass_imgs[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN with attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### define blocks, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormedConvSA_Block(eqx.Module):\n",
    "    conv: eqx.nn.Conv2d\n",
    "    pool: eqx.nn.MaxPool2d\n",
    "    drop: eqx.nn.Dropout\n",
    "    batch_norm1: eqx.nn.BatchNorm\n",
    "    batch_norm2: eqx.nn.BatchNorm\n",
    "    attention: eqx.nn.MultiheadAttention\n",
    "    input_shape: tuple\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_shape: Sequence[int] = (3, 32, 32),\n",
    "        out_channels: int = 16,\n",
    "        conv_window: int = 3,\n",
    "        pooling_kernel: int = 2,\n",
    "        pooling_stride: int = 1,\n",
    "        drop_rate: int = 0.0,\n",
    "        key: jax.Array = random.key(42),\n",
    "    ):\n",
    "        \"\"\"Block with the following layers:\n",
    "        2D convolution -> 2D pooling -> RELU activation -> batch norm -> self-attention -> batch norm\n",
    "\n",
    "        Args:\n",
    "            input_shape (Sequence[int], optional): Shape of (unbatched) input. Order is Channel, Height, Width. Defaults to (3,32,32).\n",
    "            out_channels (int, optional): Number of output channels. Defaults to 16.\n",
    "            conv_window (int, optional): Size of 2d convolutional window. Defaults to 3.\n",
    "            pooling_kernel (int, optional): Kernel for 2d max pooling. Defaults to 2.\n",
    "            pooling_stride (int, optional): Stride length for pooling. Defaults to 1.\n",
    "            drop_rate (int, optional): Dropout rate. Defaults to 0.0.\n",
    "            key (jax.Array, optional): PRNGKey. Defaults to random.key(42).\n",
    "        \"\"\"          \n",
    "        key1, key2 = random.split(key)\n",
    "        self.conv = eqx.nn.Conv2d(\n",
    "            input_shape[0], out_channels, kernel_size=conv_window, key=key1\n",
    "        )\n",
    "        if pooling_kernel == pooling_stride == 1:\n",
    "            self.pool = eqx.nn.Identity()\n",
    "        else:\n",
    "            self.pool = eqx.nn.MaxPool2d(pooling_kernel, stride=pooling_stride)\n",
    "        self.drop = eqx.nn.Dropout(drop_rate)\n",
    "\n",
    "        new_H = (\n",
    "            int(((input_shape[1] - conv_window) + 1 - pooling_kernel) / pooling_stride)\n",
    "            + 1\n",
    "        )\n",
    "        self.input_shape = (out_channels, new_H, new_H)\n",
    "        self.batch_norm1 = eqx.nn.BatchNorm(out_channels, \"batch\", momentum=0.5)\n",
    "        self.batch_norm2 = eqx.nn.BatchNorm(out_channels, \"batch\", momentum=0.5)\n",
    "        # self.batch_norm1 = eqx.nn.LayerNorm(self.input_shape[1:])\n",
    "        # self.batch_norm2 = eqx.nn.LayerNorm(self.input_shape[1:])\n",
    "\n",
    "        self.attention = eqx.nn.MultiheadAttention(\n",
    "            1, new_H**2, key=key2, dropout_p=drop_rate\n",
    "        )\n",
    "\n",
    "    def __call__(self, x, key: jax.Array, state=None, train=True):\n",
    "        key1, key2 = random.split(key)\n",
    "        x = self.conv(x)\n",
    "        x = self.pool(x)\n",
    "        x = jax.nn.gelu(x)\n",
    "        x = self.drop(x, key=key1, inference=not train)\n",
    "        x, state = self.batch_norm1(x, state, inference=not train)\n",
    "        # x = jax.vmap(self.batch_norm1)(x)\n",
    "        x_q = jnp.reshape(\n",
    "            x, (self.input_shape[0], self.input_shape[1] * self.input_shape[2])\n",
    "        )\n",
    "        attn = self.attention(x_q, x_q, x_q, key=key2, inference=not train)\n",
    "        attn = jnp.reshape(\n",
    "            attn, (self.input_shape[0], self.input_shape[1], self.input_shape[2])\n",
    "        )\n",
    "        # x = jax.vmap(self.batch_norm2)(x + attn)\n",
    "        x, state = self.batch_norm2(x+attn, state, inference=not train)\n",
    "        return x, state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class eqx_CNN_SA_Classifier(eqx.Module):\n",
    "    conv_layers: Sequence[NormedConvSA_Block]\n",
    "    linear_layers: Sequence[NormedDense_Block]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_shape: Sequence[int] = (3, 32, 32),\n",
    "        num_classes: int = 10,\n",
    "        conv_channels: Sequence[int] = [16],\n",
    "        conv_windows: Sequence[int] = [5],\n",
    "        pooling_kernel: Sequence[int] = [2],\n",
    "        pooling_stride: Sequence[int] = [2],\n",
    "        conv_drop: float = 0.0,\n",
    "        hidden_dims: Sequence[int] = [64],\n",
    "        dense_drop: float = 0.0,\n",
    "        key: jax.Array = random.key(42),\n",
    "    ):\n",
    "        \"\"\"Classifier formed by stacking convolutional+self attention and dense blocks.\n",
    "\n",
    "    Args:\n",
    "        input_shape (Sequence[int], optional): (Unbatched) shape of input. Order is Channel, Height, Width. Defaults to (3, 32, 32).\n",
    "        num_classes (int, optional): Number of classes to predict. Shape of final dense output. Defaults to 10.\n",
    "        conv_channels (Sequence[int], optional): Number of channels for each convolutional layer. Defaults to [16].\n",
    "        conv_windows (Sequence[int], optional): Window size for each convolutional layer. Defaults to [5].\n",
    "        pooling_kernel (Sequence[int], optional): Kernel size for each convolutional layer. Defaults to [2].\n",
    "        pooling_stride (Sequence[int], optional): Stride length for each convolutional layer. Defaults to [2].\n",
    "        conv_drop (float, optional): Dropout rate for convolutional layers. Defaults to 0.0.\n",
    "        hidden_dims (Sequence[int], optional): Hidden dims for intermediate dense layers. Defaults to [64].\n",
    "        dense_drop (float, optional): Dropout for dense layers. Defaults to 0.0.\n",
    "        key (jax.Array, optional): PRNGKey. Defaults to random.key(42).\n",
    "        \"\"\"        \n",
    "        def assert_equal_length(*sequences):\n",
    "            # Calculate the lengths of all sequences\n",
    "            lengths = [len(seq) for seq in sequences]\n",
    "\n",
    "            # Check if all lengths are equal to the first length in the list\n",
    "            assert all(\n",
    "                length == lengths[0] for length in lengths\n",
    "            ), f\"Sequences defining ConvSA block are of different lengths: {lengths}\"\n",
    "\n",
    "        assert_equal_length(conv_channels, conv_windows, pooling_kernel, pooling_stride)\n",
    "\n",
    "        # establish convulutional layers\n",
    "        self.conv_layers = []\n",
    "        key, *conv_keys = random.split(key, num=len(conv_channels) + 1)\n",
    "        conv_iterator = enumerate(\n",
    "            zip(conv_windows, conv_channels, pooling_kernel, pooling_stride, conv_keys)\n",
    "        )\n",
    "        for i, (cw, cf, pk, ps, k) in conv_iterator:\n",
    "            self.conv_layers.append(\n",
    "                NormedConvSA_Block(\n",
    "                    input_shape=input_shape,\n",
    "                    out_channels=cf,\n",
    "                    conv_window=cw,\n",
    "                    pooling_kernel=pk,\n",
    "                    pooling_stride=ps,\n",
    "                    key=k,\n",
    "                    drop_rate=conv_drop,\n",
    "                )\n",
    "            )\n",
    "\n",
    "            # necessary to track input shapes for each layer, follows basic convolution shape formula\n",
    "            new_H = (\n",
    "                int(\n",
    "                    ((input_shape[1] - conv_windows[i]) + 1 - pooling_kernel[i])\n",
    "                    / pooling_stride[i]\n",
    "                )\n",
    "                + 1\n",
    "            )\n",
    "            input_shape = (cf, new_H, new_H)\n",
    "\n",
    "        # establish linear layers\n",
    "        self.linear_layers = []\n",
    "        key, *lin_keys = random.split(key, num=len(hidden_dims) + 1)\n",
    "        input_shape = np.prod(input_shape)\n",
    "        for i, (hidden_dim, k) in enumerate(zip(hidden_dims, lin_keys)):\n",
    "            self.linear_layers.append(\n",
    "                NormedDense_Block(\n",
    "                    in_dim=input_shape, out_dim=hidden_dim, key=k, drop_rate=dense_drop\n",
    "                )\n",
    "            )\n",
    "            input_shape = hidden_dim\n",
    "        self.linear_layers.append(\n",
    "            NormedDense_Block(\n",
    "                in_dim=input_shape, out_dim=num_classes, key=key, drop_rate=0\n",
    "            )\n",
    "        )\n",
    "\n",
    "    @eqx.filter_jit\n",
    "    def __call__(self, x, key, state=None, train=True):\n",
    "        subkey, *conv_keys = random.split(key, num=len(self.conv_layers) + 1)\n",
    "        for layer, k in zip(self.conv_layers, conv_keys):\n",
    "            x, state = layer(x, k, state=state, train=train)\n",
    "        x = jnp.ravel(x)\n",
    "        subkey, *lin_keys = random.split(subkey, num=len(self.linear_layers) + 1)\n",
    "        for layer, k in zip(self.linear_layers, lin_keys):\n",
    "            x, state = layer(x, k, state=state, train=train)\n",
    "        x = jax.nn.log_softmax(x)\n",
    "        return x, state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "\n",
    "class CNN_SAClassTrainer(EqxTrainerModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        conv_windows: Sequence[int],\n",
    "        conv_channels: Sequence[int],\n",
    "        pooling_kernel: Sequence[bool],\n",
    "        pooling_stride: Sequence[bool],\n",
    "        hidden_dims: Sequence[int],\n",
    "        input_shape: int,\n",
    "        num_classes: int,\n",
    "        conv_drop: float = 0.0,\n",
    "        dense_drop: float = 0.0,\n",
    "        trial: Any = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self.trial = trial\n",
    "        super().__init__(\n",
    "            model_class=eqx_CNN_SA_Classifier,\n",
    "            model_hparams={\n",
    "                \"conv_windows\": conv_windows,\n",
    "                \"conv_channels\": conv_channels,\n",
    "                \"pooling_kernel\": pooling_kernel,\n",
    "                \"pooling_stride\": pooling_stride,\n",
    "                \"input_shape\": input_shape,\n",
    "                \"hidden_dims\": hidden_dims,\n",
    "                \"num_classes\": num_classes,\n",
    "                \"conv_drop\": conv_drop,\n",
    "                \"dense_drop\": dense_drop,\n",
    "            },\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "    def create_functions(self):\n",
    "        def loss_function(model, x, y, key, state, train=True):\n",
    "            model_out, state = jax.vmap(\n",
    "                model,\n",
    "                axis_name=\"batch\",\n",
    "                in_axes=(0, None, None, None),\n",
    "                out_axes=(0, None),\n",
    "            )(x, key, state, train)\n",
    "            # since we use log_softmax, cross entropy is just the value of our model output\n",
    "            # indexed by the true value of y.\n",
    "            loss = jnp.take_along_axis(model_out, jnp.expand_dims(y, 1), axis=1)\n",
    "            loss = -jnp.mean(loss)\n",
    "\n",
    "            pred_y = jnp.argmax(model_out, axis=1)\n",
    "            acc = jnp.mean(y == pred_y)\n",
    "            return loss, (acc, state)\n",
    "\n",
    "        def train_step(model, opt_state, batch, key, state):\n",
    "            x, y = batch\n",
    "            (loss, (acc, state)), grads = eqx.filter_value_and_grad(\n",
    "                loss_function, has_aux=True\n",
    "            )(model, x, y, key, state)\n",
    "            updates, opt_state = self.opt.update(grads, opt_state, model)\n",
    "            model = eqx.apply_updates(model, updates)\n",
    "            metrics = {\"loss\": loss, \"acc\": acc}\n",
    "            return model, opt_state, metrics, state\n",
    "\n",
    "        def eval_step(model, batch, key, state):\n",
    "            x, y = batch\n",
    "            _, (acc, _) = loss_function(model, x, y, key, state, train=False)\n",
    "            return {\"acc\": acc}\n",
    "\n",
    "        return train_step, eval_step\n",
    "\n",
    "    def on_validation_epoch_end(self, epoch_idx, eval_metrics, val_loader):\n",
    "        if self.trial:\n",
    "            self.trial.report(eval_metrics[\"val/acc\"], step=epoch_idx)\n",
    "            if self.trial.should_prune():\n",
    "                raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    def bind_model(self):\n",
    "        \"\"\"\n",
    "        Returns a batched model with parameters bound to it. Enables an easier inference\n",
    "        access.\n",
    "\n",
    "        Returns:\n",
    "          The model with parameters and evt. batch statistics bound to it.\n",
    "        \"\"\"\n",
    "\n",
    "        def mod(x):\n",
    "            model_out, state = jax.vmap(\n",
    "                self.model,\n",
    "                axis_name=\"batch\",\n",
    "                in_axes=(0, None, None, None),\n",
    "                out_axes=(0, None),\n",
    "            )(x, self.key, self.state, False)\n",
    "            return model_out\n",
    "\n",
    "        return mod\n",
    "\n",
    "    def vizualize_grad_norms(self, batch):\n",
    "        \"\"\"Plots histogram of gradient norms of model leaves\"\"\"\n",
    "\n",
    "        def loss_function(model, x, y, key, state, train=True):\n",
    "            model_out, state = jax.vmap(\n",
    "                model,\n",
    "                axis_name=\"batch\",\n",
    "                in_axes=(0, None, None, None),\n",
    "                out_axes=(0, None),\n",
    "            )(x, key, state, train)\n",
    "            # since we use log_softmax, cross entropy is just the value of our model output\n",
    "            # indexed by the true value of y.\n",
    "            loss = jnp.take_along_axis(model_out, jnp.expand_dims(y, 1), axis=1)\n",
    "            loss = -jnp.mean(loss)\n",
    "\n",
    "            pred_y = jnp.argmax(model_out, axis=1)\n",
    "            acc = jnp.mean(y == pred_y)\n",
    "            return loss, (acc, state)\n",
    "\n",
    "        def get_grad_and_path(model, batch, key, state):\n",
    "            x, y = batch\n",
    "            (loss, (acc, state)), grads = eqx.filter_value_and_grad(\n",
    "                loss_function, has_aux=True\n",
    "            )(model, x, y, key, state)\n",
    "            dt = jax.tree_util.tree_leaves_with_path(grads)\n",
    "            grads, names = [], []\n",
    "            for (\n",
    "                path,\n",
    "                grad,\n",
    "            ) in (\n",
    "                dt\n",
    "            ):  # filter basic info we don't care about (bias terms, normalization)\n",
    "                if (\n",
    "                    (len(grad.shape) > 1)\n",
    "                    and (path[-1].name != \"bias\")\n",
    "                    and (not path[-2].name.endswith(\"norm\"))\n",
    "                ):\n",
    "                    grads.append(grad.reshape(-1))\n",
    "                    names.append(path[-2].name + \"_\" + path[-1].name)\n",
    "            return grads, names\n",
    "\n",
    "        def viz_grads(grads, names, color=\"C0\"):\n",
    "            columns = len(grads)\n",
    "            fig, ax = plt.subplots(1, columns, figsize=(columns * 3.5, 2.5))\n",
    "            for g_idx, g in enumerate(grads):\n",
    "                key = f\"{names[g_idx]}\"\n",
    "                key_ax = ax[g_idx % columns]\n",
    "                sns.histplot(data=g, bins=30, ax=key_ax, color=color, kde=True)\n",
    "                key_ax.set_title(str(key))\n",
    "                key_ax.set_xlabel(\"Grad magnitude\")\n",
    "            fig.suptitle(\"Gradient magnitude distributions\", fontsize=14, y=1.05)\n",
    "            fig.subplots_adjust(wspace=0.45)\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "\n",
    "        grads, names = get_grad_and_path(self.model, batch, self.key, self.state)\n",
    "        viz_grads(grads, names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer2 = CNN_SAClassTrainer(\n",
    "    conv_windows=[5],\n",
    "    conv_channels=[16],\n",
    "    pooling_kernel=[2],\n",
    "    pooling_stride=[2],\n",
    "    hidden_dims=[64],\n",
    "    input_shape=(3, 32, 32),\n",
    "    num_classes=10,\n",
    "    conv_drop=0.0,\n",
    "    dense_drop=0.25,\n",
    "    optimizer_hparams={\"optimizer\": \"nadamw\", \"lr\": 5e-3},\n",
    "    logger_params={\n",
    "        \"base_log_dir\": CHECKPOINT_PATH,\n",
    "    },\n",
    "    check_val_every_n_epoch=5,\n",
    "    seed=42,\n",
    "    debug=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics2 = trainer2.train_model(train_loader, val_loader, test_loader, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Validation accuracy: {metrics2[\"val/acc\"]:4.2%}')\n",
    "print(f'Test accuracy: {metrics2[\"test/acc\"]:4.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_loader))\n",
    "trainer2.vizualize_grad_norms(batch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### optuna hyperparam val example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    my_train_loader, my_val_loader = create_data_loaders(\n",
    "        train_set, val_set, train=[True, False], batch_size=200\n",
    "    )\n",
    "    trainer = CNNClassTrainer(\n",
    "        conv_windows=[3, 5],\n",
    "        conv_channels=[16, 32],\n",
    "        pooling_kernel=[2, 2],\n",
    "        pooling_stride=[2, 2],\n",
    "        hidden_dims=[64],\n",
    "        input_shape=(3, 32, 32),\n",
    "        num_classes=10,\n",
    "        conv_drop=0.0,\n",
    "        dense_drop=trial.suggest_float(\"drop\", 0.1, 0.6, log=True),\n",
    "        optimizer_hparams={\n",
    "            \"optimizer\": \"nadamw\",\n",
    "            \"weight_decay\": trial.suggest_float(\"weight_decay\", 1e-5, 1e-1, log=True),\n",
    "            \"lr\": trial.suggest_float(\"lr\", 1e-5, 5e-2, log=True),\n",
    "        },\n",
    "        logger_params={\"base_log_dir\": CHECKPOINT_PATH},\n",
    "        check_val_every_n_epoch=5,\n",
    "        trial=trial,\n",
    "    )\n",
    "    metrics = trainer.train_model(my_train_loader, my_val_loader, num_epochs=25)\n",
    "    del trainer\n",
    "    del my_train_loader, my_val_loader\n",
    "    return metrics[\"val/acc\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(\n",
    "    study_name=\"mlp_cifar10\",\n",
    "    storage=f'sqlite:///{os.path.join(CHECKPOINT_PATH, trainer.config[\"model_class\"] ,\"optuna_hparam_search_eqn.db\")}',\n",
    "    direction=\"maximize\",\n",
    "    pruner=optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=10),\n",
    "    load_if_exists=True,\n",
    ")\n",
    "study.optimize(objective, n_trials=25 - len(study.trials), n_jobs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial = study.best_trial\n",
    "print(f\"Best Validation Accuracy: {trial.value:4.2%}\")\n",
    "print(f\"Best Params:\")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"-> {key}: {value}\")\n",
    "\n",
    "fig = optuna.visualization.plot_intermediate_values(study)\n",
    "fig.show()\n",
    "fig = optuna.visualization.plot_param_importances(study)\n",
    "fig.show()\n",
    "fig = optuna.visualization.plot_contour(study, params=[\"lr\", \"weight_decay\"])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
